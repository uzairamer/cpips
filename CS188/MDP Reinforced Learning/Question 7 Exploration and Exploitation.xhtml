<!-- Ahmed Ismail Khalid								Zulqarnain
	 150829												150913
	 BSCS VI-B											BSCS VI-B
     ahmedik95316@gmail.com								mustafa.zulqarnain123@gmail.com
     150829@students.au.edu.pk							150909@students.au.edu.pk 
     sheikh.qasim@gmail.com    +923008540838	 

				EDX Course : Artificial Intelligence				Topic : Reinforcement Learning
 -->

<problem>

<p>For each of the following action-selection methods, indicate which option describes it best.</p>

<br/>

<b>Part A</b>
<p>With probability \(p\), select \(argmax_a Q(s,a)\). With probability \(1-p\), select a random action. \(p=0.99\)</p>


<multiplechoiceresponse>
  <choicegroup type="MultipleChoice">
    <choice correct="true">Mostly exploration</choice>
    <choice correct="false">Mostly exploitation</choice>
    <choice correct="false">Mix of both</choice>
  </choicegroup>
  <solution>
      <div class="detailed-solution">
        <p>One of the several schemes for forcing exploration is a simple scheme called <b>\(\epsilon\)</b> greedy uses random actions. If the probabilty of the action is small (<b>\(\epsilon\)</b>), then the act is randomly choosem. However, if the probability is large (1 - <b>\(\epsilon\)</b>), then the current policy is choosen to act on</p>
      </div>
    </solution>
</multiplechoiceresponse>

<br/><br/><br/><br/>

<b>Part B</b>

<p>Select action a with probability \[ P(a|s) = \frac{e^{Q(s,a)/\tau}}{\sum_{a'} e^{Q(s,a')/\tau}} \] where \(\tau\) is a temperature parameter that is decreased over time.</p>
<multiplechoiceresponse>
  <choicegroup type="MultipleChoice">
    <choice correct="true">Mostly exploration</choice>
    <choice correct="false">Mostly exploitation</choice>
    <choice correct="false">Mix of both</choice>
  </choicegroup>
</multiplechoiceresponse>

<br/><br/><br/><br/>

<b>Part C</b>

<p>Always select a random action.</p>
<multiplechoiceresponse>
  <choicegroup type="MultipleChoice">
    <choice correct="false">Mostly exploration</choice>
    <choice correct="true">Mostly exploitation</choice>
    <choice correct="false">Mix of both</choice>
  </choicegroup>
</multiplechoiceresponse>

<br/><br/><br/><br/>

<b>Part D</b>

<p>Keep track of a count, \(K_{s,a}\), for each state-action tuple, (s,a), of the number of times that tuple has been seen and select \(argmax_a [Q(s,a) - K_{s,a}]\).</p>
<multiplechoiceresponse>
  <choicegroup type="MultipleChoice">
    <choice correct="false">Mostly exploration</choice>
    <choice correct="true">Mostly exploitation</choice>
    <choice correct="false">Mix of both</choice>
  </choicegroup>
</multiplechoiceresponse>

<br/><br/><br/><br/>

<b>Part E</b>

<p>Which method(s) would be advisable to use when doing Q-Learning?</p>
<choiceresponse>
  <checkboxgroup>
    <choice correct="false">A</choice>
    <choice correct="true">B</choice>
    <choice correct="false">C</choice>
    <choice correct="false">D</choice>
  </checkboxgroup>
</choiceresponse>


</problem>